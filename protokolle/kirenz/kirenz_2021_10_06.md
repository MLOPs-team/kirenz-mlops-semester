# Termin mit Prof. Kirenz am 06.10.2021

- Bosch interessiert vor allem auch die Frage: "Wie kann man mit den Daten systematisch umgehen?", z.B. Metadatenmanagement

## Empfohlenes Vorgehen

Prinzipiell sollen wir am besten Delta Lake nutzen. Dort gibt es eine Schritt-Für-Schritt-Anleitung für den ganzen Prozess

### Delta Lake

- Ist Open Source
- State of the Art bei MLOPs 

Vorgehen: 

- In Erfahrung bringen welche Daten wir nutzen können
- Daten als batch abziehen und dann eine Technologie nutzen um daraus bewegte Daten zu simulieren (Apache Airflow/Nifi)
- In welcher Form speichere ich die Daten ab (Lakehouse -> Apache HBase anschauen, das ist eine Open Source Alternative zu BigQuery)
- Wie gestalte ich das Metadatenmanagement
- Wie werden die Daten bereit gestellt

Das gesamte Vorgehen ist von Delta Lake abgebildet. Deshalb hat er das auf jeden Fall empfohlen. Wenn man Delta Lake nicht nutzt muss man das alles selber implementieren/entwickeln

### Sonstiges
- Modell muss nicht nutzbar für 10 verschiedene Use Cases sein. Es geht darum, eine Systemarchitektur aufzubauen in der man einzelne Bausteine austauschen kann. Die Pipeline selbst soll dabei weitestgehend automatisiert sein. 
- Wir sollen am besten eine Datenquelle mit Kriminalitätsdaten aus Deutschland finden!!
- Zum Deployment: Am besten TFX (von Google) anschauen. Dort gibt es mehrere Optionen für das Deployment. Kubeflow kann auch den gesamten Prozess abbilden, das ist auch schon mit Kubernetes verbunden und deshalb beliebig skalierbar
- Für die Dashboards sollen wir Plotly (auf React-Basis) nutzen. Das ist HTML basiert und deshalb besser als matplotlib  

